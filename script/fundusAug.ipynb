{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f04683",
   "metadata": {},
   "outputs": [],
   "source": [
    "####  initialization\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "%reset\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "import copy\n",
    "\n",
    "print(sys.version)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "\n",
    "import dnnlib\n",
    "import legacy\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "# torch.cuda.set_device(1)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print('CUDA is available: ',torch.cuda.is_available())\n",
    "mpth = '../weights/network-snapshot-005400.pkl'\n",
    "with dnnlib.util.open_url(mpth) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
    "\n",
    "with dnnlib.util.open_url(mpth) as f:\n",
    "    g_style = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "debug_image = True\n",
    "calculate_grad = False\n",
    "\n",
    "calculate_corres = True\n",
    "use_backward_grad = True\n",
    "\n",
    "# load_grad = False # if need load, then True, if have loaded, then False\n",
    "load_corre = False\n",
    "\n",
    "use_lesion_mask = True\n",
    "\n",
    "generate_from_w = True\n",
    "\n",
    "\n",
    "filename = 'test4043'\n",
    "img_w_latent = filename\n",
    "z_filename = '../latents/{}/projected_w.npy'.format(img_w_latent)\n",
    "\n",
    "# num = '_z2822_16_7459_02'\n",
    "# grad_filename = 'tmp/gradnpy/grad/grads_{}'.format(filename)\n",
    "# corre_filename = 'tmp/gradnpy/grad/corres_{}'.format(filename)\n",
    "plt.figure(figsize=(5,5),dpi=256)\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "############################  lesion segment  ##########################################\n",
    "########################################################################################\n",
    "def lesion_pre_processing(img):\n",
    "    import cv2\n",
    "    cliplimit = 2\n",
    "    gridsize = 8\n",
    "    image = img.clone().squeeze(0)\n",
    "    image = image.cpu().numpy()\n",
    "\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    black_mask = np.uint8((image_gray > 15)*255.)\n",
    "    ret, thresh = cv2.threshold(black_mask, 127, 255, 0)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask = np.ones(image.shape[:2], dtype='uint8')*255\n",
    "    cn = []\n",
    "    for contour in contours:\n",
    "        if len(contour) > len(cn):\n",
    "            cn = contour\n",
    "    cv2.drawContours(mask, [cn], -1, 0, -1)\n",
    "    ## mask\n",
    "    \n",
    "    \n",
    "    # brightness balance.\n",
    "    brightnessbalance = False\n",
    "    if brightnessbalance:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        mask_img = mask\n",
    "        brightness = gray.sum() / (mask_img.shape[0] * mask_img.shape[1] - mask_img.sum()/255.)\n",
    "        image = np.uint8(np.minimum(image * brightnessbalance / brightness, 255))\n",
    "\n",
    "    # illumination correction and contrast enhancement.\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    lab_planes = list(cv2.split(lab))\n",
    "    clahe = cv2.createCLAHE(clipLimit=cliplimit,tileGridSize=(gridsize,gridsize))\n",
    "    lab_planes[0] = clahe.apply(lab_planes[0])\n",
    "    lab = cv2.merge(lab_planes)\n",
    "    nimg = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    denoise = True\n",
    "    if denoise:\n",
    "        nimg = cv2.fastNlMeansDenoisingColored(nimg, None, 10, 10, 1, 3)\n",
    "        nimg = cv2.bilateralFilter(nimg, 5, 1, 1)\n",
    "\n",
    "    \n",
    "    # plt.subplot(223)\n",
    "    # plt.imshow(nimg)\n",
    "    nimg = torch.from_numpy(nimg).unsqueeze(0).to(device)\n",
    "    return nimg\n",
    "\n",
    "sys.path.append('../ThirdPart/HEDNet_cGAN/')\n",
    "from transform.transforms_group import *\n",
    "def lesion_seg(image):\n",
    "    from optparse import OptionParser\n",
    "    import random\n",
    "    import copy\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    from torch import optim\n",
    "    from torch.optim import lr_scheduler\n",
    "    import config_gan_ex as config\n",
    "    from hednet import HNNNet\n",
    "    from dnet import DNet\n",
    "    from utils import get_images\n",
    "    from dataset import IDRIDDataset\n",
    "    from torchvision import datasets, models, transforms    \n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    import argparse\n",
    "\n",
    "    image_size = config.IMAGE_SIZE\n",
    "    image_dir = config.IMAGE_DIR\n",
    "    softmax = nn.Softmax(1)\n",
    "    def eval_model(model, image):\n",
    "        model.eval()\n",
    "        masks_soft = []\n",
    "        masks_hard = []\n",
    "        m_transform = transforms.Compose([\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            inputs = image.permute(0,3,1,2)\n",
    "            maxv = torch.max(inputs)\n",
    "            minv = torch.min(inputs)\n",
    "            absmax = max(maxv,abs(minv))\n",
    "            inputs = inputs/absmax  ## \n",
    "            inputs = m_transform(inputs)\n",
    "\n",
    "            inputs = inputs.to(device=device, dtype=torch.float)\n",
    "            bs, _, h, w = inputs.shape\n",
    "            # not ignore the last few patches\n",
    "            h_size = (h - 1) // image_size + 1\n",
    "            w_size = (w - 1) // image_size + 1\n",
    "            masks_pred = torch.zeros(inputs.shape).to(dtype=torch.float)[:,:2,:,:]\n",
    "            print(h_size,w_size,'h_w_size')\n",
    "            for i in range(h_size):\n",
    "                for j in range(w_size):\n",
    "                    h_max = min(h, (i + 1) * image_size)\n",
    "                    w_max = min(w, (j + 1) * image_size)\n",
    "                    inputs_part = inputs[:,:, i*image_size:h_max, j*image_size:w_max]\n",
    "                    masks_pred_single = model(inputs_part)[-1]\n",
    "                    masks_pred[:, :, i*image_size:h_max, j*image_size:w_max] = masks_pred_single\n",
    "                    ## \n",
    "            masks_pred_softmax_batch = softmax(masks_pred).cpu().numpy()\n",
    "            masks_soft_batch = masks_pred_softmax_batch[:, 1:, :, :]\n",
    "        return masks_soft_batch[0][0]\n",
    "    tseed = 1234\n",
    "    model_pth = '../weights/model_True.pth.tar'\n",
    "    lesion_type = 'EX'\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(tseed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(tseed)\n",
    "    np.random.seed(tseed)\n",
    "    random.seed(tseed)\n",
    "\n",
    "    model = HNNNet(pretrained=True, class_number=2)\n",
    "\n",
    "    resume = model_pth\n",
    "\n",
    "    if os.path.isfile(resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "        checkpoint = torch.load(resume,map_location=device)\n",
    "        start_epoch = checkpoint['epoch']+1\n",
    "        start_step = checkpoint['step']\n",
    "        try:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        except:\n",
    "            model.load_state_dict(checkpoint['g_state_dict'])\n",
    "        print('Model loaded from {}'.format(resume))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "    model.to(device)\n",
    "    pre_image = lesion_pre_processing(image)\n",
    "\n",
    "    if debug_image:\n",
    "        img = pre_image.to(torch.uint8)\n",
    "        im3 = Image.fromarray(img[0].cpu().numpy(), 'RGB').resize((512,512))\n",
    "        plt.subplot(2,2,3)\n",
    "        plt.imshow(im3)\n",
    "        # im3.save('out/vesseltest/'+'testshow.png')\n",
    "    mask = eval_model(model, pre_image)\n",
    "        \n",
    "    if debug_image:\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.imshow((mask>0.5).astype('uint8'))\n",
    "        masksave = (mask>0.5).astype('uint8')*255\n",
    "        masksave = Image.fromarray(masksave).resize((512,512))\n",
    "        # masksave.save('out/vesseltest/'+'testshowlesion.png')\n",
    "\n",
    "    ## (0~1)\n",
    "    return (mask>0.5).astype('uint8')\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "############################  backward hook  ###########################################\n",
    "########################################################################################\n",
    "\n",
    "total_grad = []\n",
    "count = 1\n",
    "def backward_hook(module,grad_in,grad_out):\n",
    "    global count\n",
    "    if count == 1:\n",
    "        print(type(grad_out),len(grad_out),type(grad_out[0]),grad_out[0].shape,grad_out[0][0].shape)\n",
    "    count += 1\n",
    "    total_grad.append(torch.abs(grad_out[0][0]))\n",
    "    # total_grad.append(grad_out[0][0].detach().cpu())\n",
    "\n",
    "if \"back_handle\" in locals():\n",
    "    for h in back_handle:\n",
    "        h.remove()   \n",
    "back_handle = []\n",
    "for name,block in G.named_modules():\n",
    "    if 'affine' in name:\n",
    "        cur_handle = block.register_backward_hook(backward_hook)\n",
    "        back_handle.append(cur_handle)\n",
    "\n",
    "########################################################################################\n",
    "############################  Generate image  ##########################################\n",
    "########################################################################################\n",
    "\n",
    "#### load from z space or w+ space\n",
    "z_size = G.z_dim\n",
    "latent_code_pth = z_filename\n",
    "if os.path.exists(latent_code_pth):\n",
    "    latent_zs = np.load(latent_code_pth)\n",
    "    if generate_from_w:\n",
    "        latent_zs = latent_zs\n",
    "    else:\n",
    "        latent_zs = latent_zs.squeeze(0)[:1]\n",
    "else:\n",
    "    latent_zs = np.random.randn(1, G.z_dim)\n",
    "latent_zs = torch.from_numpy(latent_zs.astype(np.float32))\n",
    "z = latent_zs.to(device)\n",
    "    \n",
    "\n",
    "z.requires_grad_(True)\n",
    "label = torch.zeros([1, G.c_dim], device=device)\n",
    "class_idx = None\n",
    "if G.c_dim != 0:\n",
    "    if class_idx is None:\n",
    "        ctx.fail('Must specify class label with --class when using a conditional network')\n",
    "    label[:, class_idx] = 1\n",
    "else:\n",
    "    if class_idx is not None:\n",
    "        print ('warn: --class=lbl ignored when running on an unconditional network')\n",
    "\n",
    "if generate_from_w:\n",
    "    image = G.synthesis(z, noise_mode='const')\n",
    "else:\n",
    "    image = G(z, label,truncation_psi=1.0)\n",
    "img = (image.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "im2 = Image.fromarray(img[0].cpu().numpy(), 'RGB').resize((512,512))\n",
    "if debug_image:\n",
    "    plt.subplot(221)\n",
    "    plt.imshow(im2)\n",
    "\n",
    "########################################################################################\n",
    "############################  gradient backward  #######################################\n",
    "########################################################################################\n",
    "if use_lesion_mask:\n",
    "    mask = lesion_seg(img)\n",
    "if use_backward_grad:\n",
    "    mask = torch.from_numpy(mask).to(device)\n",
    "    mask = mask.repeat((1,3,1,1))\n",
    "    image.backward(mask, retain_graph=True)\n",
    "\n",
    "# ################################# for ~mask areas ###############################################\n",
    "n_total_grad = []\n",
    "n_count = 1\n",
    "def n_backward_hook(module,grad_in,grad_out):\n",
    "    global n_count\n",
    "    n_count += 1\n",
    "    n_total_grad.append(torch.abs(grad_out[0][0]))\n",
    "    # total_grad.append(grad_out[0][0].detach().cpu())\n",
    "# G.zero_grad()\n",
    "if \"n_back_handle\" in locals():\n",
    "    for h in n_back_handle:\n",
    "        h.remove()   \n",
    "n_back_handle = []\n",
    "for name,block in g_style.named_modules():\n",
    "    if 'affine' in name:\n",
    "        cur_handle = block.register_backward_hook(n_backward_hook)\n",
    "        n_back_handle.append(cur_handle)\n",
    "        \n",
    "\n",
    "if use_backward_grad:\n",
    "    latent_code_pth = z_filename\n",
    "    if os.path.exists(latent_code_pth):\n",
    "        latent_zs = np.load(latent_code_pth)\n",
    "        if generate_from_w:\n",
    "            latent_zs = latent_zs\n",
    "        else:\n",
    "            latent_zs = latent_zs.squeeze(0)[:1]\n",
    "    else:\n",
    "        latent_zs = np.random.randn(1, G.z_dim)\n",
    "    latent_zs = torch.from_numpy(latent_zs.astype(np.float32))\n",
    "    zz = latent_zs.to(device)\n",
    "    \n",
    "    zz.requires_grad_(True)\n",
    "    if generate_from_w:\n",
    "        nimg = g_style.synthesis(zz,noise_mode='const')\n",
    "    else:\n",
    "        nimg = g_style(zz, label,truncation_psi=1.0)\n",
    "    nmask = 1-mask\n",
    "    nimg.backward(nmask, retain_graph=True)\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "############################  correction calculate  ####################################\n",
    "########################################################################################\n",
    "# print(\"total_cost:\",time.perf_counter() - start_time)\n",
    "print(\"total_output_size:\", len(total_grad) / len(back_handle),len(back_handle))\n",
    "print(\"n_total_output_size:\", len(n_total_grad) / len(n_back_handle),len(n_back_handle))\n",
    "\n",
    "intersections = []\n",
    "affine_nums = len(back_handle)\n",
    "# img_size_flat = output_resize.shape[-1]*output_resize.shape[-2]\n",
    "# grad_size = (output_resize.shape[-2],output_resize.shape[-1])\n",
    "# mask_size = (mask.shape[-2],mask.shape[-1])\n",
    "\n",
    "num_affine_layers = len(back_handle)\n",
    "print('num_affine_layers',num_affine_layers)\n",
    "O = mask.sum()\n",
    "P = O/(mask.shape[-2]*mask.shape[-1])\n",
    "gradient_threshold = 150 ### 0~255\n",
    "discount_factor = 2\n",
    "if calculate_corres:\n",
    "    for layer in range(num_affine_layers):\n",
    "        channels = grad_shape = total_grad[layer].shape[0]\n",
    "        # print(channels,' all channels nums...',layer)\n",
    "        for channel in range(channels):\n",
    "            if use_backward_grad:\n",
    "                \n",
    "                v_grad = total_grad[layer][channel].detach().cpu().numpy()\n",
    "                rv_grad = n_total_grad[layer][channel].detach().cpu().numpy()##  gradient for ~mask\n",
    "                v_grad = 0.0 if np.isnan(v_grad) else v_grad\n",
    "                rv_grad = 0.0 if np.isnan(rv_grad) else rv_grad\n",
    "                # corre =  (v_grad - rv_grad)/O\n",
    "                # corre =  (v_grad)/(O.cpu().numpy())\n",
    "                corre = 10*(v_grad)/(O.cpu().numpy()) + (rv_grad)/(255.0-O.cpu().numpy())\n",
    "                t = [layer,channel,corre]\n",
    "                intersections.append(t)\n",
    "\n",
    "    intersections = sorted(intersections,key=lambda iterm  : float('-inf') if math.isnan(iterm[2]) else iterm[2],reverse=True)   \n",
    "    print(len(intersections),len(intersections[0]))\n",
    "\n",
    "\n",
    "print(len(total_grad),total_grad[0].shape)\n",
    "\n",
    "########################################################################################\n",
    "############################  forward editing  #########################################\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "def forward_hook_wrap(cur_index,cur_channel,debug=True):\n",
    "    def forward_hook(module,input,output):\n",
    "        if debug:\n",
    "            print('current edit layer and channels are: ',cur_index,cur_channel)\n",
    "        output[0,cur_channel] = output[0,cur_channel]-1.0*torch.max(output)\n",
    "        return output\n",
    "    return forward_hook\n",
    "    \n",
    "\n",
    "if \"handle\" in locals():\n",
    "    for h in handle:\n",
    "        h.remove()\n",
    "handle = []\n",
    "\n",
    "\n",
    "select_mode = 2 ##[0:best_num 1:best_list 2:layer_channels]\n",
    "best_num = 1\n",
    "select_list=[0]\n",
    "select_lc = [[5,33]] ####[layer,channel]\n",
    "if select_mode == 2:\n",
    "    select_list = []\n",
    "    for (index,v) in enumerate(intersections):\n",
    "        for lc in select_lc:\n",
    "            if lc[0] == v[0] and lc[1] == v[1]:\n",
    "                select_list.append(index)\n",
    "                print(index)\n",
    "                break\n",
    "corre_all_array = np.array(intersections)\n",
    "if select_mode == 0:\n",
    "    corre_array = corre_all_array[:best_num]\n",
    "    edit_l = corre_array[:best_num,0]\n",
    "    edit_c = corre_array[:best_num,1]\n",
    "elif select_mode == 1 or select_mode == 2:\n",
    "    corre_array = corre_all_array[select_list]\n",
    "    edit_l = corre_array[:,0]\n",
    "    edit_c = corre_array[:,1]\n",
    "\n",
    "layer_count = 15\n",
    "for name,block in g_style.named_modules():\n",
    "    # print(name)\n",
    "    if 'affine' in name:\n",
    "        if layer_count in edit_l:\n",
    "            edit_channel = edit_c[layer_count==edit_l]\n",
    "            cur_handle = block.register_forward_hook(forward_hook_wrap(layer_count,edit_channel))\n",
    "            handle.append(cur_handle)\n",
    "        layer_count -= 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    if generate_from_w:\n",
    "        new_image = g_style.synthesis(z, noise_mode='const')\n",
    "    else:\n",
    "        new_image= g_style(z, label,truncation_psi=1.0)\n",
    "    new_img = (new_image.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "    new_im = Image.fromarray(new_img[0].cpu().squeeze().numpy(), 'RGB').resize((256,256)) \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(new_im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25471020",
   "metadata": {},
   "outputs": [],
   "source": [
    "### higest contribution score\n",
    "### please select the layer after 5th !!!! (5-7)\n",
    "##########################################################################################\n",
    "### paper \"GAN Dissection: Visualizing and Understanding Generative Adversarial Networks\"\n",
    "### and paper \"Image2StyleGAN++: How to Edit the Embedded Images?\"\n",
    "### show that the diversity of units matching high-level object concepts peaks at middle layer\n",
    "### then declines in later layers, with the later layers dominated by textures, colors,and shapes\n",
    "##########################################################################################\n",
    "corres = np.array(intersections)\n",
    "# index = (corres[:,0] > 5) & (corres[:,1] > 50) & (corres[:,1] < 512-50)\n",
    "index = (corres[:,0] > 3) & (corres[:,1] > 50) & (corres[:,1] < 512-50)\n",
    "##### sometimes maybe not the first one, you can also try second or third one.\n",
    "(hlayer,hchannel,_) = corres[index][0] ### corres[index][1]\n",
    "hlayer = int(hlayer)\n",
    "hchannel = int(hchannel)\n",
    "# corres[:20] ########## show top 20 \n",
    "print('layer and channel with higest contribution score : ', hlayer,hchannel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75741f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "9\n",
      "index layers [4.00000000e+00 8.60000000e+01 2.08930241e-02]\n",
      "editing in [4.] [86.]\n",
      "Finished !!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x1280 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################### show different editing strength ##############\n",
    "##################### for editing  #################################\n",
    "\n",
    "from torch.autograd.gradcheck import get_analytical_jacobian,_iter_tensors,_compute_analytical_jacobian_rows,_stack_and_check_tensors\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "########################################################################################\n",
    "############################  forward editing  #########################################\n",
    "########################################################################################\n",
    "plt.figure(figsize=(5,5),dpi=256)\n",
    "\n",
    "def forward_hook_wrap(cur_index,cur_channel,edit_strenth=0.0,debug=False):\n",
    "    def forward_hook(module,input,output):\n",
    "        if debug:\n",
    "            print('current edit layer and channels are: ',cur_index,cur_channel)\n",
    "            # print('output.shape..',output[0].shape)\n",
    "        output[0,cur_channel] = output[0,cur_channel]+edit_strenth*torch.max(output)\n",
    "        return output\n",
    "    return forward_hook\n",
    "    \n",
    "# 清除当前style\n",
    "if \"handle\" in locals():\n",
    "    for h in handle:\n",
    "        h.remove()\n",
    "handle = []\n",
    "\n",
    "\n",
    "select_mode = 2 ##[0:best_num 1:best_list 2:layer_channels]\n",
    "best_num = 1\n",
    "select_list=[0]\n",
    "t_layer = hlayer\n",
    "t_channel = hchannel\n",
    "positive = 0\n",
    "# select_lc = [[t_layer,t_channel],[5,139]] ####[layer,channel]\n",
    "select_lc = [[t_layer,t_channel]] ####[layer,channel]\n",
    "# corres = np.array(intersections)\n",
    "# index = 6 == corres[:,0] \n",
    "# select_lc = corres[index][:5]\n",
    "print(type(select_lc))\n",
    "if select_mode == 2:\n",
    "    select_list = []\n",
    "    for (index,v) in enumerate(intersections):\n",
    "        for lc in select_lc:\n",
    "            if lc[0] == v[0] and lc[1] == v[1]:\n",
    "                select_list.append(index)\n",
    "                print(index)\n",
    "                break\n",
    "corre_all_array = np.array(intersections)\n",
    "if select_mode == 0:\n",
    "    corre_array = corre_all_array[:best_num]\n",
    "    print('index layers',corre_array[0])\n",
    "    edit_l = corre_array[:best_num,0]\n",
    "    edit_c = corre_array[:best_num,1]\n",
    "elif select_mode == 1 or select_mode == 2:\n",
    "    corre_array = corre_all_array[select_list]\n",
    "    print('index layers',corre_array[0])\n",
    "    edit_l = corre_array[:,0]\n",
    "    edit_c = corre_array[:,1]\n",
    "    print('editing in',edit_l,edit_c)\n",
    "\n",
    "    \n",
    "\n",
    "# with open(z_filename, 'rb') as f:\n",
    "#     z = pickle.load(f).to(device)\n",
    "\n",
    "latent_code_pth = z_filename\n",
    "if os.path.exists(latent_code_pth):\n",
    "    latent_zs = np.load(latent_code_pth)\n",
    "    if generate_from_w:\n",
    "        latent_zs = latent_zs\n",
    "    else:\n",
    "        latent_zs = latent_zs.squeeze(0)[:1]\n",
    "else:\n",
    "    latent_zs = np.random.randn(1, G.z_dim)\n",
    "latent_zs = torch.from_numpy(latent_zs.astype(np.float32))\n",
    "z = latent_zs.to(device)\n",
    "\n",
    "save_video = 1\n",
    "show_img = 0\n",
    "save_evaluation_img = 0\n",
    "result_name = 'testvideo'\n",
    "if save_video:\n",
    "    from matplotlib.patches import Rectangle\n",
    "    from matplotlib.widgets import Slider\n",
    "    # from moviepy.video.io.bindings import mplfig_to_npimage\n",
    "    import io\n",
    "fps = 30\n",
    "if save_video:\n",
    "    video = imageio.get_writer(f'{result_name}.mp4', mode='I', fps=fps, codec='libx264', bitrate='16M')\n",
    "\n",
    "\n",
    "if  save_evaluation_img:\n",
    "    # save_dir = os.path.join('out','edited',img_w_latent) \n",
    "    save_dir = os.path.join('out','evaluation','images','editstrength')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    im2.save(os.path.join(save_dir,'ori',img_w_latent+'.png'))\n",
    "    \n",
    "\n",
    "\n",
    "RangeEdit = 100\n",
    "EachStep = 5\n",
    "for index,edit_strength in enumerate(range(0,RangeEdit+1,EachStep)):\n",
    "# for index,edit_strength in enumerate(range(0,-(RangeEdit+1),-EachStep)):\n",
    "    edit_strength = edit_strength*0.005\n",
    "    # 清除当前style\n",
    "    if \"handle\" in locals():\n",
    "        for h in handle:\n",
    "            h.remove()\n",
    "    handle = []\n",
    "    layer_count = 15\n",
    "    for name,block in g_style.named_modules():\n",
    "        # print(name)\n",
    "        if 'affine' in name:\n",
    "            if layer_count in edit_l:\n",
    "                edit_channel = edit_c[layer_count==edit_l]\n",
    "                cur_handle = block.register_forward_hook(forward_hook_wrap(layer_count,edit_channel,edit_strength))\n",
    "                handle.append(cur_handle)\n",
    "            layer_count -= 1\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if generate_from_w:\n",
    "            new_image = g_style.synthesis(z,noise_mode='const')\n",
    "        else:\n",
    "            new_image = g_style(z, label,truncation_psi=1.0)\n",
    "        new_img = (new_image.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "        new_im = Image.fromarray(new_img[0].cpu().squeeze().numpy(), 'RGB').resize((512,512)) \n",
    "        if show_img:\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(new_im)\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(im2)\n",
    "            \n",
    "        if  save_evaluation_img and not (index % 10):\n",
    "            # save_dir = os.path.join('out','edited',img_w_latent) \n",
    "            # os.makedirs(save_dir, exist_ok=True)\n",
    "            new_im.save(os.path.join(save_dir,str(index),'edit_l'+str(t_layer)+'_c'+str(t_channel)+'_e'+str(index)+'.png'))\n",
    "\n",
    "        \n",
    "        if save_video:\n",
    "            plt.gcf()\n",
    "            fig = plt.figure(figsize=(8,8),dpi=200,facecolor='white')\n",
    "            plt.axis('off')\n",
    "            plt.imshow(np.concatenate([im2, new_im], axis=1))\n",
    "            # plt.text(420,40, 'Increase', fontsize=14,color='white')\n",
    "            # plt.text(682,550, 'decrease', fontsize=15)\n",
    "            # plt.text(150, 550, 'Reference', fontsize=15)\n",
    "            # plt.text(682,550, 'Editing', fontsize=15)\n",
    "            ax = plt.gca()\n",
    "            # Create a Rectangle patch\n",
    "            rect = Rectangle((850,220),170,200,linewidth=1,edgecolor='r',facecolor='none')\n",
    "            # Add the patch to the Axes\n",
    "            ax.add_patch(rect)\n",
    "            # Make a horizontal slider to control the frequency.\n",
    "            \n",
    "            axfreq = fig.add_axes([0.17, 0.27, 0.65, 0.03])\n",
    "            axfreq.set_facecolor('white')\n",
    "            # ax.set_facecolor('black')\n",
    "            # plt.text(0.35, -0.5, 'Editing Strength', fontsize=11)\n",
    "\n",
    "            vinit = round(EachStep*index/float(RangeEdit),2)\n",
    "            freq_slider = Slider(\n",
    "                ax=axfreq,\n",
    "                label='[α]',\n",
    "                valmin=0,\n",
    "                valmax=1,\n",
    "                valinit=vinit,\n",
    "            )\n",
    "            \n",
    "            # plt.savefig('out/fig.png',bbox_inches='tight')\n",
    "            img_buf = io.BytesIO()\n",
    "            plt.savefig(img_buf, format='png',bbox_inches='tight')\n",
    "            videoframe = np.array(Image.open(img_buf).resize((1280,720)))\n",
    "            plt.close()\n",
    "            # videoframe = np.resize(videoframe,(1280,720))\n",
    "            \n",
    "            # plt.show()\n",
    "            if edit_strength == 0:\n",
    "                for edit_0 in range(1,30):\n",
    "                    video.append_data(videoframe)\n",
    "            video.append_data(videoframe)\n",
    "\n",
    "if save_video:\n",
    "    print('Finished !!!')\n",
    "    video.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43dec61f-4605-4ad4-91c2-09e903107d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"testvideo.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"testvideo.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36810977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"testvideo_1.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd58ebd09ea77a12e5e98aee5b3425cf9f6f1a2f863516f23d676cee1aaa8098"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
